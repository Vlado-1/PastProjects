{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "moderate-texas",
   "metadata": {},
   "source": [
    "# <center>Workload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-shadow",
   "metadata": {},
   "source": [
    "### <center>Recommend top 5 mention users to each tweet user in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-booking",
   "metadata": {},
   "source": [
    "Key points about problem:\n",
    "* The degree to which a mention user should be recommended to a given tweet user will depend on the number of times they appear in the tweet.\n",
    "* A collaboriative filtering ML algorithm (called ALS) will need to be trained and applied on all the data\n",
    "* User: tweet users\n",
    "* Item: mention users\n",
    "* Rating: number of times the tweet user mentions the mention user\n",
    "* The ALS algorithm has a function to display top N recommendations for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "liked-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import IntegerType\n",
    "from operator import add\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "realistic-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Workload 2 - top 5 recommendations\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "path = \"./tweets.json\"\n",
    "tweets = spark.read.json(path, multiLine=True)\n",
    "\n",
    "# Drop unecessary columns\n",
    "tweets = tweets.drop(\"created_at\")\n",
    "tweets = tweets.drop(\"hash_tags\")\n",
    "tweets = tweets.drop(\"text\")\n",
    "tweets = tweets.drop(\"replyto_user_id\")\n",
    "tweets = tweets.drop(\"retweet_user_id\")\n",
    "tweets = tweets.drop(\"replyto_id\")\n",
    "tweets = tweets.drop(\"retweet_id\")\n",
    "tweets = tweets.drop(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-membrane",
   "metadata": {},
   "source": [
    "### 1.0 Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sound-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out null user mentions\n",
    "tweets = tweets.filter(\"user_mentions is not null\")\n",
    "\n",
    "# Create new records (\"user_id mention_user_id\", rating) for each user mentioned in a tweet\n",
    "def unroll_user_mentions(rec):\n",
    "    return [( str(rec[0]) + \" \" + str(user_mention[0]), len(user_mention[1])) for user_mention in rec[1]]\n",
    "\n",
    "tweets_rdd = tweets.rdd.flatMap(unroll_user_mentions)\n",
    "\n",
    "# Multiple tweets may have the same user_id and mention_user_id.\n",
    "# So sum the rating values across these tweets.\n",
    "tweets_rdd = tweets_rdd.reduceByKey(add)\n",
    "\n",
    "# Separate the user_id and mention_user_id from the same string.\n",
    "# Create new records (user_id, mention_user_id, rating)\n",
    "tweets_rdd = tweets_rdd.map(lambda rec: (rec[0].split(\" \")[0],rec[0].split(\" \")[1],rec[1]))\n",
    "\n",
    "schema = [\"user_id\", \"mention_user\", \"rating\"]\n",
    "newTweets = spark.createDataFrame(tweets_rdd, schema)\n",
    "#newTweets.filter(\"rating > 2\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-celebrity",
   "metadata": {},
   "source": [
    "### 2.0 Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nervous-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit: Stack overflow on how to do label encoding for categorical values\n",
    "# https://stackoverflow.com/questions/30580410/how-to-do-labelencoding-or-categorical-value-in-apache-spark/31027848\n",
    "user_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_index\")\n",
    "mention_indexer = StringIndexer(inputCol=\"mention_user\", outputCol=\"mention_user_index\")\n",
    "\n",
    "newTweets = user_indexer.fit(newTweets).transform(newTweets)\n",
    "newTweets = mention_indexer.fit(newTweets).transform(newTweets)\n",
    "newTweets = newTweets.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-polyester",
   "metadata": {},
   "source": [
    "### 3.0 Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lasting-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the ALS collaborative filtering algorithm\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_index\", itemCol=\"mention_user_index\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(newTweets)\n",
    "\n",
    "# Recommend the top 5 mention users for each tweet user\n",
    "userRecs = model.recommendForAllUsers(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-orlando",
   "metadata": {},
   "source": [
    "### 4.0 Post-Processing and Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "southeast-mambo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-------------------+----------------+-------------------+\n",
      "|            user_id|   Recommendation 1|   Recommendation 2|   Recommendation 3|Recommendation 4|   Recommendation 5|\n",
      "+-------------------+-------------------+-------------------+-------------------+----------------+-------------------+\n",
      "|           96812612|           17464397|         2876041031|1003107003693137921|       866065134|          421223083|\n",
      "|          124102246|            9300262|           39511166|           25049056|        85583894|           61534021|\n",
      "|1384402650536419331|            9300262|           39511166|           25049056|        85583894|           61534021|\n",
      "|           76890214|           25049056|            9300262|           39511166|        85583894|           61534021|\n",
      "|1068842909968023552|           17464397|         2876041031|1003107003693137921|       866065134|          421223083|\n",
      "|         2905571287|            9300262|           39511166|           25049056|        85583894|           61534021|\n",
      "|          772912315|            9300262|           39511166|           25049056|        85583894|           61534021|\n",
      "|           46369866|          449027230|           15859912|         3419251203|      1092378031|          991940041|\n",
      "|          217100313|           17464397|         2876041031|1003107003693137921|       866065134|          421223083|\n",
      "|           22614847|           17464397|         2876041031|1003107003693137921|       866065134|          421223083|\n",
      "|          196881864|1155838084522618881| 707212147240128512| 751283882457128961|        15012486| 789232424194572288|\n",
      "|           85284402|           17464397|         2876041031|1003107003693137921|       866065134|          421223083|\n",
      "|1197862862510678016|           17464397|         2876041031|1003107003693137921|       866065134|          421223083|\n",
      "| 928889002861752321|           55060090| 712671893237731328|           24259259|        40851610|1321935792416149505|\n",
      "|          341454994|          449027230|           15859912|         3419251203|      1092378031|          991940041|\n",
      "|          128674057|           24259259|1155838084522618881|           92555364|        96900937| 751283882457128961|\n",
      "|1317461961538502656|            9300262|           39511166|           25049056|        85583894|           61534021|\n",
      "|            7795922|           17464397|         2876041031|1003107003693137921|       866065134|          421223083|\n",
      "|1034330380239687680|            9300262|           39511166|           25049056|        85583894|           61534021|\n",
      "|         2992117173|           17464397|         2876041031|1003107003693137921|       866065134|          421223083|\n",
      "+-------------------+-------------------+-------------------+-------------------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display top 5 mention users for each tweet user\n",
    "#\n",
    "# Step 1: Unroll top 5 encoded mention users.\n",
    "def unroll_recommendations(record):\n",
    "    return [(record[0], recommendation[\"mention_user_index\"], recommendation[\"rating\"]) for recommendation in record[1]]\n",
    "userRecs = userRecs.rdd.flatMap(unroll_recommendations)\n",
    "userRecs = spark.createDataFrame(userRecs,[\"user_id_index\", \"mention_user_index\", \"rating\"])\n",
    "\n",
    "# Step 2: Unencode user_ids and mention_user_ids by joining with cached tweets table\n",
    "newTweets_no_user = newTweets.drop('rating', \"user_id_index\", \"user_id\").drop_duplicates(subset=['mention_user'])\n",
    "userRecs = userRecs.join(newTweets_no_user, on=\"mention_user_index\").drop(\"mention_user_index\")\n",
    "\n",
    "newTweets_no_mention = newTweets.drop('mention_user', 'rating', 'mention_user_index').drop_duplicates(subset=['user_id'])\n",
    "userRecs = userRecs.join(newTweets_no_mention, on=\"user_id_index\").drop(\"user_id_index\", \"rating\")\n",
    "\n",
    "# Step 3: Group mention users and ratings by by user_id\n",
    "userRecs_rdd = userRecs.rdd.map(lambda rec: (rec[1], rec[0]))\n",
    "userRecs_rdd = userRecs_rdd.groupByKey()\n",
    "\n",
    "# Step 4: Format grouped mention users into individual columns\n",
    "def unroll_top_5(values):\n",
    "    top5 = []    \n",
    "    for v in values:\n",
    "            top5.append(v)\n",
    "    return top5\n",
    "userRecs_rdd = userRecs_rdd.mapValues(unroll_top_5)\n",
    "userRecs_rdd = userRecs_rdd.map(lambda rec: (rec[0], rec[1][0], rec[1][1], rec[1][2], rec[1][3], rec[1][4]))\n",
    "\n",
    "# Step 5: Display\n",
    "userRecs = spark.createDataFrame(userRecs_rdd, [\"user_id\", \"Recommendation 1\", \"Recommendation 2\", \"Recommendation 3\", \"Recommendation 4\", \"Recommendation 5\" ])\n",
    "userRecs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wound-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
